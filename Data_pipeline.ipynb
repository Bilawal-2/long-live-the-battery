{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data format</b><br>\n",
    "1 Row = 1 Example = 1 Cycle<br>\n",
    "Each cycle so far has:\n",
    " - Qdlin (1000,1)\n",
    " - Tdlin (1000,1)\n",
    " - Cdlin (1000,1) (WIP)\n",
    " - discharge_time (1,) (WIP)\n",
    " - IR (1,)\n",
    " - remaining_cycle_life (1,) <- target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# only taking batch1 for testing\n",
    "path1 = Path(\"Data/batch1.pkl\")\n",
    "batch1 = pickle.load(open(path1, 'rb'))\n",
    "\n",
    "# remove batteries that do not reach 80% capacity\n",
    "del batch1['b1c8']\n",
    "del batch1['b1c10']\n",
    "del batch1['b1c12']\n",
    "del batch1['b1c13']\n",
    "del batch1['b1c22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "see Hands-On Machine Learning pp.416\n",
    "\n",
    "1. The get_cycle_features function fetches all features and targets from \n",
    "the batch1 file and convert to \"Example\" objects. Every Example contains \n",
    "data from one charging cycle.\n",
    "\n",
    "2. Create a \"Data/tfrecords\" directory.\n",
    "\n",
    "3. For each cell create a tfrecord file with the naming convention \"b1c0.tfrecord\".\n",
    "The SerializeToString method creates binary data out of the Example objects that can\n",
    "be read natively in TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "def get_cycle_features(cell, idx):\n",
    "    cycle_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"IR\": Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]])),\n",
    "                \"Qdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"])),\n",
    "                \"Tdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"])),\n",
    "                \"Remaining_cycles\": Feature(int64_list=Int64List(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return cycle_example\n",
    "\n",
    "\n",
    "data_dir = \"Data/tfrecords\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "        num_cycles = int(batch1[cell][\"cycle_life\"])-1\n",
    "        for cycle in range(num_cycles):\n",
    "            cycle_to_write = get_cycle_features(cell, cycle)\n",
    "            f.write(cycle_to_write.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.feature_column import numeric_column, make_parse_example_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns for our dataset\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000])\n",
    "rem_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "columns = [ir, qdlin, tdlin, rem_cycles]\n",
    "\n",
    "\"\"\"\n",
    "Reading the remaining code from bottom to top:\n",
    "\n",
    "When writing to TFrecord we created one file for each cell. Now we merge the\n",
    "data back into one dataset and prepare it to be fed directly into a model.\n",
    "\n",
    "The interleave() method will create a dataset that pulls 4 file paths from the\n",
    "filepath_dataset and for each one calls the function \"read_tfrecords\". It will then\n",
    "cycle through these 4 datasets, reading one line at a time from each until all datasets\n",
    "are out of items. Then it gets the next 4 file paths from the filepath_dataset and\n",
    "interleaves them the same way, and so on until it runs out of file paths. \n",
    "Note: Even with parallel calls specified, data within batches is still sequential.\n",
    "\n",
    "The read_tfrecords() function reads a file, skipping the first row which in our case\n",
    "is 0/NaN most of the time. It then loops over each example/row in the dataset and\n",
    "calls the parse_feature function. Then it batches the dataset, so it always feeds\n",
    "multiple examples at the same time, and then shuffles the batches. It is important \n",
    "that we batch before shuffling, so the examples within the batches stay in order.\n",
    "\n",
    "The parse_features function takes an example and converts it from binary/message format\n",
    "into a more readable format. The make_parse_example_spec generates a feature mapping \n",
    "according to the columns we defined. To be able to feed the dataset directly into a\n",
    "Tensorflow model later on, we need to split the data into examples and targets (i.e. X and y).\n",
    "\"\"\"\n",
    "\n",
    "def parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, make_parse_example_spec(columns))\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    data = tf.data.TFRecordDataset(file).skip(1)\n",
    "    data = data.map(parse_features).batch(5).shuffle(1000)\n",
    "    data = data.prefetch(1) # this is only relevant for CPU to GPU pipelines, see Hands-On ML p.411\n",
    "    return data\n",
    "\n",
    "# define files to read from and store in a list_files object\n",
    "filepaths = [os.path.join(\"Data/tfrecords/\" + cell + \".tfrecord\") for cell in batch1.keys()] \n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=4, num_parallel_calls=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for examples, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed dataset into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import DenseFeatures, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All we need is a DenseFeatures layer that specifies the input columns.\n",
    "See Hands-On ML p.426\n",
    "\n",
    "This does not work with an RNN layer instead of Dense(18) yet.\n",
    "\"\"\"\n",
    "input_columns = columns[:-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(DenseFeatures(feature_columns=input_columns))\n",
    "model.add(Dense(18))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES AND ALTERNATIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two more ways to read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = \"b1c0\"\n",
    "# skip(1) is important to keep the first record of each cell out, which is all Nan/0\n",
    "raw_dataset = tf.data.TFRecordDataset([\"Data/tfrecords/\" + cell + \".tfrecord\"]).skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1\n",
    "# reading manually\n",
    "\n",
    "feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32),\n",
    "    \"Qdlin\": tf.io.FixedLenFeature(shape=[1000], dtype=tf.float32),\n",
    "    \"Tdlin\": tf.io.FixedLenFeature(shape=[1000], dtype=tf.float32),\n",
    "    \"Remaining_cycles\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "}\n",
    "    \n",
    "def _parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "dataset = raw_dataset.map(_parse_features).batch(5).shuffle(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2\n",
    "# reading with feature columns\n",
    "\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000])\n",
    "rem_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "columns = [ir, qdlin, tdlin, rem_cycles]\n",
    "\n",
    "def _parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, make_parse_example_spec(columns))\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "dataset = raw_dataset.map(_parse_features).batch(5).shuffle(1000).repeat(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for examples, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing example with FeatureList\n",
    "\n",
    "Summarizing the Xdlin-features with FeatureList might be helpful if we wanted to keep information about the sequence data on the detail-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList\n",
    "from tensorflow.train import Feature, Features, FeatureList, FeatureLists, SequenceExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write \n",
    "cell = batch1[\"b1c0\"]\n",
    "\n",
    "ir = Feature(float_list=FloatList(value=[cell[\"summary\"][\"IR\"][1]]))\n",
    "qdlin = Feature(float_list=FloatList(value=cycle[\"Qdlin\"]))\n",
    "tdlin = Feature(float_list=FloatList(value=cycle[\"Tdlin\"]))\n",
    "\n",
    "detail_features = FeatureList(feature=[qdlin, tdlin])\n",
    "\n",
    "cycle_example = SequenceExample(\n",
    "    context = Features(feature={\"IR\":ir}),\n",
    "    feature_lists = FeatureLists(feature_list={\"Details\":detail_features})\n",
    ")\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_aligned_cycle.tfrecord\") as f:\n",
    "    f.write(cycle_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "context_feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "}\n",
    "\n",
    "sequence_feature_description = {\n",
    "    \"Details\": tf.io.FixedLenSequenceFeature([1000], tf.float32),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_aligned_cycle.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_feature_description,\n",
    "        sequence_feature_description\n",
    "    )\n",
    "    print(parsed_example[1][\"Details\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
