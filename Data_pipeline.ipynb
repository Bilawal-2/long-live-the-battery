{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data format</b><br>\n",
    "1 Row = 1 Example = 1 Cycle<br>\n",
    "Each cycle so far has:\n",
    " - Qdlin (1000,1)\n",
    " - Tdlin (1000,1)\n",
    " - Cdlin (1000,1) (WIP)\n",
    " - discharge_time (1,) (WIP)\n",
    " - IR (1,)\n",
    " - remaining_cycle_life (1,) <- target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# only taking batch1 for testing\n",
    "path1 = Path(\"Data/batch1.pkl\")\n",
    "batch1 = pickle.load(open(path1, 'rb'))\n",
    "\n",
    "# remove batteries that do not reach 80% capacity\n",
    "del batch1['b1c8']\n",
    "del batch1['b1c10']\n",
    "del batch1['b1c12']\n",
    "del batch1['b1c13']\n",
    "del batch1['b1c22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a349aa36d53a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tfrecord\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch1' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "see Hands-On Machine Learning pp.416\n",
    "\n",
    "1. The get_cycle_features function fetches all features and targets from \n",
    "the batch1 file and convert to \"Example\" objects. Every Example contains \n",
    "data from one charging cycle.\n",
    "\n",
    "2. Create a \"Data/tfrecords\" directory.\n",
    "\n",
    "3. For each cell create a tfrecord file with the naming convention \"b1c0.tfrecord\".\n",
    "The SerializeToString method creates binary data out of the Example objects that can\n",
    "be read natively in TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "def get_cycle_example(cell, idx):\n",
    "    cycle_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"IR\": Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]])),\n",
    "                \"Qdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"])),\n",
    "                \"Tdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"])),\n",
    "                \"Remaining_cycles\": Feature(int64_list=Int64List(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return cycle_example\n",
    "\n",
    "\n",
    "data_dir = \"Data/tfrecords/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "        num_cycles = int(batch1[cell][\"cycle_life\"])-1\n",
    "        for cycle in range(num_cycles):\n",
    "            cycle_to_write = get_cycle_example(cell, cycle)\n",
    "            f.write(cycle_to_write.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.feature_column import numeric_column, make_parse_example_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you haven't loaded the batch data yet and want to read from TFRecord files, \n",
    "# run this cell instead to get an index and replace \"batch1.keys()\" with \"batch1_keys\" in the cell below\n",
    "batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4', 'b1c5', 'b1c6', 'b1c7', 'b1c9', 'b1c11', 'b1c14', 'b1c15', 'b1c16', 'b1c17', 'b1c18', 'b1c19', 'b1c20', 'b1c21', 'b1c23', 'b1c24', 'b1c25', 'b1c26', 'b1c27', 'b1c28', 'b1c29', 'b1c30', 'b1c31', 'b1c32', 'b1c33', 'b1c34', 'b1c35', 'b1c36', 'b1c37', 'b1c38', 'b1c39', 'b1c40', 'b1c41', 'b1c42', 'b1c43', 'b1c44', 'b1c45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define columns for our dataset\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000])\n",
    "rem_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "columns = [ir, qdlin, tdlin, rem_cycles]\n",
    "\n",
    "\"\"\"\n",
    "Reading the remaining code from bottom to top:\n",
    "\n",
    "When writing to TFrecord we created one file for each cell. Now we merge the\n",
    "data back into one dataset and prepare it to be fed directly into a model.\n",
    "\n",
    "The interleave() method will create a dataset that pulls 4 file paths from the\n",
    "filepath_dataset and for each one calls the function \"read_tfrecords\". It will then\n",
    "cycle through these 4 datasets, reading one line at a time from each until all datasets\n",
    "are out of items. Then it gets the next 4 file paths from the filepath_dataset and\n",
    "interleaves them the same way, and so on until it runs out of file paths. \n",
    "Note: Even with parallel calls specified, data within batches is still sequential.\n",
    "\n",
    "The read_tfrecords() function reads a file, skipping the first row which in our case\n",
    "is 0/NaN most of the time. It then loops over each example/row in the dataset and\n",
    "calls the parse_feature function. Then it batches the dataset, so it always feeds\n",
    "multiple examples at the same time, and then shuffles the batches. It is important \n",
    "that we batch before shuffling, so the examples within the batches stay in order.\n",
    "\n",
    "The parse_features function takes an example and converts it from binary/message format\n",
    "into a more readable format. The make_parse_example_spec generates a feature mapping \n",
    "according to the columns we defined. To be able to feed the dataset directly into a\n",
    "Tensorflow model later on, we need to split the data into examples and targets (i.e. X and y).\n",
    "\"\"\"\n",
    "window_size = 5\n",
    "\n",
    "def parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, make_parse_example_spec(columns))\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "def flatten_windows(features, target):\n",
    "    features = features.batch(window_size)\n",
    "    target = target.skip(window_size-1)\n",
    "    row = tf.data.Dataset.zip((features, target))\n",
    "    return row\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    dataset = tf.data.TFRecordDataset(file).skip(1) # skip can be removed when we have clean data\n",
    "    dataset = dataset.map(parse_features)\n",
    "    #dataset = dataset.window(window_size, 1, 1, True).flat_map(flatten_windows)\n",
    "    #dataset = dataset.shuffle(1000).batch(10).prefetch(1) # prefetch is only relevant for CPU to GPU pipelines, see Hands-On ML p.411\n",
    "    return dataset\n",
    "\n",
    "# define files to read from and store in a list_files object\n",
    "filepaths = [os.path.join(\"Data/tfrecords/\" + cell + \".tfrecord\") for cell in batch1_keys] \n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=1, num_parallel_calls=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(755, shape=(), dtype=int64)\n",
      "tf.Tensor(754, shape=(), dtype=int64)\n",
      "tf.Tensor(753, shape=(), dtype=int64)\n",
      "tf.Tensor(752, shape=(), dtype=int64)\n",
      "tf.Tensor(751, shape=(), dtype=int64)\n",
      "tf.Tensor(750, shape=(), dtype=int64)\n",
      "tf.Tensor(749, shape=(), dtype=int64)\n",
      "tf.Tensor(748, shape=(), dtype=int64)\n",
      "tf.Tensor(747, shape=(), dtype=int64)\n",
      "tf.Tensor(746, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for feature, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below this line is experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed dataset into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import DenseFeatures, Dense, Activation, LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All we need is a DenseFeatures layer that specifies the input columns.\n",
    "See Hands-On ML p.426\n",
    "\n",
    "This does not work with an RNN layer instead of Dense(18) yet.\n",
    "\"\"\"\n",
    "input_columns = columns[:-1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(DenseFeatures(feature_columns=input_columns))\n",
    "model.add(Dense(18))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    247/Unknown - 2s 8ms/step - loss: 264732.4879"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f8c5e0c71664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mreset_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m           output_loss_metrics=self._output_loss_metrics)\n\u001b[0m\u001b[1;32m   1248\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, reset_metrics, output_loss_metrics)\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    296\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    238\u001b[0m                          'because it has no loss to optimize.')\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         logging.warning('The list of trainable weights is empty. Make sure that'\n\u001b[1;32m    242\u001b[0m                         \u001b[0;34m' you are not setting model.trainable to False before '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mtrainable_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0msub_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         extra_variables=self._trainable_weights)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/layer_utils.py\u001b[0m in \u001b[0;36mgather_trainable_weights\u001b[0;34m(trainable, sub_layers, extra_variables)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   trainable_extra_variables = [\n\u001b[0;32m---> 78\u001b[0;31m       v for v in extra_variables if v.trainable]\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrainable_extra_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES AND ALTERNATIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two more ways to read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = \"b1c0\"\n",
    "# skip(1) is important to keep the first record of each cell out, which is all Nan/0\n",
    "raw_dataset = tf.data.TFRecordDataset([\"Data/tfrecords/\" + cell + \".tfrecord\"]).skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1\n",
    "# reading manually\n",
    "\n",
    "feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature(shape=[], dtype=tf.float32),\n",
    "    \"Qdlin\": tf.io.FixedLenFeature(shape=[1000], dtype=tf.float32),\n",
    "    \"Tdlin\": tf.io.FixedLenFeature(shape=[1000], dtype=tf.float32),\n",
    "    \"Remaining_cycles\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64)\n",
    "}\n",
    "    \n",
    "def _parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "dataset = raw_dataset.map(_parse_features).batch(5).shuffle(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2\n",
    "# reading with feature columns\n",
    "\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000])\n",
    "rem_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "columns = [ir, qdlin, tdlin, rem_cycles]\n",
    "\n",
    "def _parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, make_parse_example_spec(columns))\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "dataset = raw_dataset.map(_parse_features).batch(5).shuffle(1000).repeat(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for examples, target in dataset.take(10):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing example with FeatureList\n",
    "\n",
    "Summarizing the Xdlin-features with FeatureList might be helpful if we wanted to keep information about the sequence data on the detail-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList\n",
    "from tensorflow.train import Feature, Features, FeatureList, FeatureLists, SequenceExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write \n",
    "cell = batch1[\"b1c0\"]\n",
    "\n",
    "ir = Feature(float_list=FloatList(value=[cell[\"summary\"][\"IR\"][1]]))\n",
    "qdlin = Feature(float_list=FloatList(value=cycle[\"Qdlin\"]))\n",
    "tdlin = Feature(float_list=FloatList(value=cycle[\"Tdlin\"]))\n",
    "\n",
    "detail_features = FeatureList(feature=[qdlin, tdlin])\n",
    "\n",
    "cycle_example = SequenceExample(\n",
    "    context = Features(feature={\"IR\":ir}),\n",
    "    feature_lists = FeatureLists(feature_list={\"Details\":detail_features})\n",
    ")\n",
    "\n",
    "with tf.io.TFRecordWriter(\"my_aligned_cycle.tfrecord\") as f:\n",
    "    f.write(cycle_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "context_feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature([], tf.float32, default_value=0)\n",
    "}\n",
    "\n",
    "sequence_feature_description = {\n",
    "    \"Details\": tf.io.FixedLenSequenceFeature([1000], tf.float32),\n",
    "}\n",
    "\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_aligned_cycle.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_feature_description,\n",
    "        sequence_feature_description\n",
    "    )\n",
    "    print(parsed_example[1][\"Details\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
