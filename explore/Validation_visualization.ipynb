{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly import graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.offline as pyo\n",
    "from google.cloud import storage\n",
    "%matplotlib inline\n",
    "pyo.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory to base, to make all imports and file paths work\n",
    "import os\n",
    "os.chdir(os.pardir)\n",
    "print(\"Current directory: %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainer.data_pipeline as dp\n",
    "import trainer.constants as cst\n",
    "from trainer.helpers import print_dict_keys, simple_plotly\n",
    "from trainer.evaluation import get_predictions_results, plot_predictions_and_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that saved_model has been created with the same version of TensorFlow\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"data/ion_age_20190614_205447/saved_model\"\n",
    "dataset_dir = cst.TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.experimental.load_from_saved_model(model_dir)\n",
    "\n",
    "# To look at available model methods:\n",
    "# list(method for method in dir(model) if not method.startswith(\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with dataset\n",
    "window_size = 20\n",
    "shift = 5\n",
    "stride = 1\n",
    "batch_size = 16\n",
    "\n",
    "dataset = dp.create_dataset(dataset_dir,\n",
    "                            window_size=window_size,\n",
    "                            shift=shift,  # Can vary during validation\n",
    "                            stride=stride,\n",
    "                            batch_size=batch_size,  # Can vary during validation\n",
    "                            cycle_length=1,  # To match original order (so no files get interleaved)\n",
    "                            num_parallel_calls=1,  # Has to be equal or below cycle_length\n",
    "                            shuffle=False,  # To match original order\n",
    "                            repeat=False)\n",
    "scaling_factors = dp.load_scaling_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_predictions_results(model, dataset, scaling_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell_index(results_df, cell_index_col_name=\"cell_index\", inplace=False):\n",
    "    # Initialization\n",
    "    if inplace:\n",
    "        results = results_df\n",
    "    else:\n",
    "        results = results_df.copy()\n",
    "    results[cell_index_col_name] = 0\n",
    "    \n",
    "    # Getting the indexes which encapsulate all cells\n",
    "    new_cell_index = list(results[results[\"target_current_cycle\"].diff() < 0].index)\n",
    "    new_cell_index.append(len(results))  # Add the last index manually, since there is no diff < 0\n",
    "    last_s = 0  # Set first starting index manually\n",
    "    \n",
    "    # Setting cell_indexes\n",
    "    for i, s in enumerate(new_cell_index):\n",
    "        results[cell_index_col_name].iloc[last_s:s] = i\n",
    "        last_s = s\n",
    "    \n",
    "    if not inplace:\n",
    "        return results\n",
    "\n",
    "results = create_cell_index(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_cycle_count_trace(results_df, window_size, cycle_bin_width, column=\"target_current_cycle\"):\n",
    "    # Get the current cycle value counts sorted from low to high\n",
    "    current_cycle_counts = (results_df[column]\n",
    "                            .value_counts()\n",
    "                            .sort_index()\n",
    "                            .reset_index()\n",
    "                            .rename(columns={\"index\": \"current_cycle_value\",\n",
    "                                             column: \"count\"}))\n",
    "\n",
    "\n",
    "    # The actual cycle counts can be {20: 42,  21: 2,  25: 42,  26: 2} since some cycles were dropped\n",
    "    # Binning aggregates these \"outliers\" with bin size equal to shift\n",
    "    bins = list(range(window_size, results_df[column].max(), cycle_bin_width))\n",
    "    bins.append(results[column].max())\n",
    "    grouped_cycle_counts = (current_cycle_counts\n",
    "                            .groupby(pd.cut(current_cycle_counts[\"current_cycle_value\"], bins=bins))\n",
    "                            .sum()\n",
    "                            .loc[:, \"count\"])\n",
    "    \n",
    "    # Convert to percent, since the absolute counts can vary widely, when cycle_bin_width changes\n",
    "    grouped_cycle_counts = ((grouped_cycle_counts - grouped_cycle_counts.min())\n",
    "                            / (grouped_cycle_counts.max() - grouped_cycle_counts.min())) * 100\n",
    "\n",
    "    return go.Scatter(\n",
    "        x = np.array(bins) - window_size,  # shift necessary to line up with error traces\n",
    "        y = grouped_cycle_counts,\n",
    "        name = \"Cells count\"\n",
    "    )\n",
    "\n",
    "def get_errors_over_cycle_traces(results_df, cycle_bin_width=100):\n",
    "    results = results_df.copy()\n",
    "\n",
    "    # Calculate absolute errors\n",
    "    results[\"ae_current_cycle\"] = (results[\"target_current_cycle\"] - results[\"pred_current_cycle\"]).abs()\n",
    "    results[\"ae_remaining_cycles\"] = (results[\"target_remaining_cycles\"] - results[\"pred_remaining_cycles\"]).abs()\n",
    "    \n",
    "    # Create bin intervalls\n",
    "    bins = list(range(0, results[\"target_current_cycle\"].max(), cycle_bin_width))\n",
    "    bins.append(results[\"target_current_cycle\"].max())\n",
    "    \n",
    "    # Aggregate mean absolute errors over bins and save as new dataframe\n",
    "    mae_binned = (results.groupby(pd.cut(results_df[\"target_current_cycle\"], bins=bins))\n",
    "                 .mean()\n",
    "                 .loc[:, [\"ae_current_cycle\", \"ae_remaining_cycles\"]])\n",
    "    \n",
    "    std_binned = (results.groupby(pd.cut(results_df[\"target_current_cycle\"], bins=bins))\n",
    "                  .std()\n",
    "                  .loc[:, [\"ae_current_cycle\", \"ae_remaining_cycles\"]])\n",
    "    \n",
    "    # Build mean absolute errors over bins\n",
    "    mae_current_cycle_trace = go.Bar(\n",
    "        x = bins,\n",
    "        y = mae_binned[\"ae_current_cycle\"],\n",
    "        name = \"mae_current_cycle\"\n",
    "    )\n",
    "    mae_remaining_cycles_trace = go.Bar(\n",
    "        x = bins,\n",
    "        y = mae_binned[\"ae_remaining_cycles\"],\n",
    "        name = \"mae_remaining_cycles\"\n",
    "    )\n",
    "    \n",
    "    # Build standard deviation of absolute errors over bins\n",
    "    std_current_cycle_trace = go.Bar(\n",
    "        x = bins,\n",
    "        y = std_binned[\"ae_current_cycle\"],\n",
    "        name = \"std_current_cycle\"\n",
    "    )\n",
    "    std_remaining_cycles_trace = go.Bar(\n",
    "        x = bins,\n",
    "        y = std_binned[\"ae_remaining_cycles\"],\n",
    "        name = \"std_remaining_cycles\"\n",
    "    )\n",
    "\n",
    "    return (mae_current_cycle_trace,\n",
    "            mae_remaining_cycles_trace,\n",
    "            std_current_cycle_trace,\n",
    "            std_remaining_cycles_trace)\n",
    "\n",
    "\n",
    "\n",
    "mae_cc, mae_rc, _, _ = get_errors_over_cycle_traces(results, 40)\n",
    "\n",
    "\n",
    "count_trace = get_binned_cycle_count_trace(results, window_size, 40)\n",
    "count_trace.update(dict(\n",
    "    mode= 'none',\n",
    "    fill='tozeroy',\n",
    "    fillcolor=\"rgba(210, 210, 210, 0.5)\",\n",
    "    yaxis=\"y2\",\n",
    "))\n",
    "\n",
    "# dtick_error = 100\n",
    "# max_error = max(mae_cc.y.max(), mae_rc.y.max())\n",
    "# tickvals_error = list(range(0, int(max_error), dtick_error))\n",
    "# print(max(tickvals_error) / max_error)\n",
    "\n",
    "layout = dict(\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    xaxis=dict(\n",
    "        title=\"Cycle\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Mean absolute error\",\n",
    "        overlaying=\"y2\",\n",
    "        tickmode=\"array\",\n",
    "        tickvals=tickvals_error,\n",
    "\n",
    "        dtick=100\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Cell count\",\n",
    "        side=\"right\",\n",
    "        tickmode=\"array\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[count_trace,\n",
    "                      mae_cc,\n",
    "                      mae_rc],\n",
    "                layout=layout)\n",
    "\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# for i, (cell_k, cell_v) in enumerate(preprocessed_pkl.items()):\n",
    "#     if i == 1:\n",
    "#         break\n",
    "#     cycle_keys = list(cell_v[\"cycles\"].keys())\n",
    "\n",
    "#     window_cycle_keys = []\n",
    "#     for i, w_slice in enumerate(range(0, len(cycle_keys), shift)):\n",
    "#         cycle_keys_slice = cycle_keys[w_slice:w_slice + window_size]\n",
    "#         if len(cycle_keys_slice) % window_size == 0:  # drop remainder\n",
    "#             window_cycle_keys.append(cycle_keys_slice)\n",
    "    \n",
    "#     assert np.all([len(w) == window_size for w in window_cycle_keys]), \\\n",
    "#         \"Not all windows have the correct window_size\"\n",
    "#     print(\"Number of windows in '{}': {}\".format(cell_k, len(window_cycle_keys)))\n",
    "#     #print(window_cycle_keys)\n",
    "#     for j, (w_cycle_keys, example) in enumerate(zip(window_cycle_keys, dataset)):  # Iterate over all windows\n",
    "#         if j == 3:\n",
    "#             break\n",
    "#         print(\"\\n##### Window {} #####\".format(j))\n",
    "        \n",
    "#         # Dataset values\n",
    "#         print(\"Example shapes:\", {k:v.shape for k, v in example[0].items()})\n",
    "#         print(\"\\nDataset Target:\", example[1].numpy())\n",
    "#         # Processed Values\n",
    "#         processed_target = cell_v[\"summary\"][cst.REMAINING_CYCLES_NAME] \\\n",
    "#                                 / scaling_factors[cst.REMAINING_CYCLES_NAME]\n",
    "#         #print(\"Processed Target:\", processed_target[(window_size + stride * j)])\n",
    "#         for z, k in enumerate(w_cycle_keys):  # Itereate over all strides\n",
    "#             print(\"\\n# Step\")\n",
    "#             scaled = example[0][cst.QDLIN_NAME][:, z, :, :].numpy().squeeze() * scaling_factors[cst.QDLIN_NAME]\n",
    "#             print(scaled.shape)\n",
    "#             processed = cell_v[\"cycles\"][k][cst.QDLIN_NAME].squeeze()\n",
    "#             print(processed.shape)\n",
    "#             print(\"Num close:, \", np.sum(np.isclose(scaled, processed, atol=1e-8)))\n",
    "#             processed_rc = (cell_v[\"summary\"][cst.REMAINING_CYCLES_NAME][z + j * (window_size - stride)] \\\n",
    "#                                 / scaling_factors[cst.REMAINING_CYCLES_NAME]).astype(np.float32)\n",
    "#             print(processed_rc)\n",
    "#             print(cell_v[\"summary\"].keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
